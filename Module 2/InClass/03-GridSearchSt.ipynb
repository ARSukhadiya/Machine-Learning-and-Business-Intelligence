{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Import the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Load the digits.csv dataset \n",
    "This is a copy of the test set of the UCI ML hand-written digits datasets https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
    "\n",
    "The data set contains images of hand-written digits: 10 classes where each class refers to a digit.\n",
    "\n",
    "Each datapoint is a 8x8 image of a digit, Classes=10, Samples per class~180, \n",
    "\n",
    "Samples total=1797, Dimensionality=64, and Features=(integers 0-16)\n",
    "\n",
    "\n",
    "- After importing this dataset, split it into test and train sets. \n",
    "- You may check the shape of data and the target attributes of the dataset. \n",
    "- You may also want to print a few samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (1438, 64)\n",
      "Shape of y_train: (1438,)\n",
      "Shape of X_test: (360, 64)\n",
      "Shape of y_test: (360,)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('digits.csv', header=None)\n",
    "\n",
    "# Separate features and target\n",
    "X = data.iloc[:, :-1]  # Features (64 columns)\n",
    "y = data.iloc[:, -1]   # Target (last column)\n",
    "\n",
    "# X, y\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of the data and target\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test. \n",
    "\n",
    "When evaluating different settings (“hyperparameters”) for estimators, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
    "\n",
    "However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n",
    "\n",
    "A solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "A model is trained using  of the folds as training data; the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.\n",
    "(Check https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Import libraries\n",
    "Import the classes `cross_val_score` and `KNeighborsClassifier` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Run cross validation\n",
    "Run cross validation for `cv=5` on the train dataset. Print the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "k = 5\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Print the mean score and confidence interval\n",
    "Print the mean score of cross validation with 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard error\n",
    "mean_score = np.mean(cv_scores)\n",
    "std_error = np.std(cv_scores) / np.sqrt(cv)\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "confidence_interval = stats.norm.interval(0.95, loc=mean_score, scale=std_error)\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean cross-validation score:\", mean_score)\n",
    "print(\"95% Confidence interval:\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Repeat Step 3 for  KFold, StratifiedKFold\n",
    "Repeat Step 3 for `KFold`, `StratifiedKFold` with number of splits `n_splits=10`. Print the mean score with 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform cross-validation and calculate mean score with confidence interval\n",
    "def evaluate_cross_validation(cv_strategy, X_train, y_train):\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv_strategy)\n",
    "    mean_score = np.mean(cv_scores)\n",
    "    std_error = np.std(cv_scores) / np.sqrt(len(cv_scores))\n",
    "    confidence_interval = stats.norm.interval(0.95, loc=mean_score, scale=std_error)\n",
    "    return mean_score, confidence_interval\n",
    "\n",
    "# KFold\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "mean_kfold, ci_kfold = evaluate_cross_validation(kfold, X_train, y_train)\n",
    "print(\"KFold Mean cross-validation score:\", mean_kfold)\n",
    "print(\"KFold 95% Confidence interval:\", ci_kfold)\n",
    "\n",
    "# StratifiedKFold\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "mean_stratified_kfold, ci_stratified_kfold = evaluate_cross_validation(stratified_kfold, X_train, y_train)\n",
    "print(\"StratifiedKFold Mean cross-validation score:\", mean_stratified_kfold)\n",
    "print(\"StratifiedKFold 95% Confidence interval:\", ci_stratified_kfold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Searches\n",
    "=================\n",
    "Exhaustive search over specified parameter values for an estimator.\n",
    "\n",
    "Important members are fit, predict.\n",
    "\n",
    "GridSearchCV implements a “fit” and a “score” method. It also implements “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.\n",
    "\n",
    "The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.\n",
    "\n",
    "See (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Grid-Search with build-in cross validation\n",
    "Import `GridSearchCV` from `sklearn.model_selection` and `SVC` from `sklearn.svm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define parameter grid:\n",
    "- `SVC` has two parameters `C` and `gamma`. \n",
    "- Do not worry about the roles of these two parameters on the algorithm. \n",
    "- Set model parameters to `'C': array([  0.001, 0.01,0.1,1,10,100])` and `gamma': array([0.00001,0.0001,0.001, 0.01, 0.1])`.\n",
    "- There are different ways of assigning parameter values, your instructor will lead you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.array([0.001, 0.01, 0.1, 1, 10, 100]),\n",
    "    'gamma': np.array([0.00001, 0.0001, 0.001, 0.01, 0.1])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Instintiate the grid search object\n",
    "Instintiate a grid search object for `SVC`, for the corresponding paranmeters with 5-folds cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Fit the grid search object on the train data set\n",
    "A GridSearchCV object behaves just like a normal classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Grid Search Results\n",
    "- Form a data frame from the grid search results.\n",
    "- Print the first 10 predictions of `X_test` using the grid search model.\n",
    "- Print the best parameter, best score and estimator of the grid search.\n",
    "- Create a heat map for the grid search parameters using the parameter values and the grid search results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Form a DataFrame from the grid search results\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# 2. Print the first 10 predictions of X_test using the grid search model\n",
    "predictions = grid_search.predict(X_test)\n",
    "print(\"First 10 Predictions:\", predictions[:10])\n",
    "\n",
    "# 3. Print the best parameters, best score, and the best estimator\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "print(\"Best Estimator:\", grid_search.best_estimator_)\n",
    "\n",
    "# 4. Create a heat map for the grid search parameters\n",
    "# Pivot the DataFrame for the heatmap\n",
    "heatmap_data = results_df.pivot(\"param_gamma\", \"param_C\", \"mean_test_score\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".3f\", cmap=\"viridis\")\n",
    "plt.title(\"Grid Search Mean Test Scores\")\n",
    "plt.xlabel(\"C Parameter\")\n",
    "plt.ylabel(\"Gamma Parameter\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
